---
title: "analyze d01"
output:
  html_document:
    self_contained: no
---

```{r knit, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, dev="svg", cache=TRUE)
```

```{r libraries}
library(tidyverse)
library(jsonlite)
library(memoise)
```

```{r theme}
source('theme.R')
```

```{r utilities}
generic.ci_ <- function(x, n = 5000, seed = 1) {
  set.seed(seed)
  structure(
    quantile(
      replicate(n, mean(sample(x, replace = TRUE))),
      c(0.025, 0.975)),
    names=c("ci.l","ci.u"))
}

generic.ci <- memoise(generic.ci_)
```



# read in data

```{r}
parsed.json = Map(f = fromJSON,
                 list.files(pattern = "*.json"))
```

# munge

```{r make-responses}
responses = do.call(
  rbind,
  Map(unname(parsed.json),
      1:length(parsed.json),
      f = function(x, subject.num) {
        
        # join examples for each rule with the metadata about the rule (id, desc, trial num)
        
        do.call(rbind,
                with(x$receive,
                     Map(1:length(id), examples, id, description,
                         f = function(trial.num, example, id, description) {
                           merge(y = cbind(example.num = 1:nrow(example),
                                           example),
                                 x = data.frame(worker.id = paste0("s", subject.num),
                                            trial.num = trial.num,
                                            rule.id = id,
                                            rule.desc = description))
                         }
                     )))
      }
  )) %>%
  rename(polarity = kind)
```

```{r compute-example-correctness}
regexes = c('3a' = 'aaa+',
            'zip-code' = '[0123456789]{5}',
            'consonants-only' = '[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]*',
            'delimiters' = "\\[.*\\]")

example.matches = function(example, rx) {
  res = regexpr(pattern = rx, text = example)
  # make sure we match and that the *entire* string is what matches, not a substring
  res > 0 & attr(res, "match.length") == nchar(example)
}
# example.correct(example = 'aaa', rx = 'aaa+')
# example.correct(example = 'baaa', rx = 'aaa+')
# example.correct(example = 'aaaa', rx = 'aaa+')

responses = responses %>%
  mutate(rx = regexes[rule.id])

responses_match = apply(responses[,c('string','rx')],
      1,
      function(e) { example.matches(example = e['string'], rx = e['rx']) })

responses = mutate(responses,
                   match = responses_match,
                   correct = !xor(polarity == 'positive', match)) %>%
  select(-rx, -rule.desc) # hide these cause they're verbose

# # testing
# View(responses %>% select(rule.id, polarity, string, correct, match) %>% arrange(rule.id, polarity))
```


# research

## how many examples do people give?

```{r, fig.width = 11, height = 2}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.examples = n()) %>%
  group_by(worker.id, rule.id, num.examples) %>%
  summarise(freq = n())

xmin = 1 #min(e.agg$num.examples)
xmax = max(e.agg$num.examples)

e.agg$num.examples.fct = factor(e.agg$num.examples, levels = as.character(xmin:xmax))


ggplot(data = e.agg) +
  theme_pub + 
  facet_grid(. ~ rule.id) +
  geom_bar(mapping = aes(x = num.examples.fct, y = freq), stat = 'identity') +
  scale_x_discrete(breaks = as.character(xmin:xmax), drop = FALSE, name = 'number of examples')
```

## how many positive examples versus negative examples?

```{r, fig.width = 11, height = 3}
e.agg = responses %>% group_by(worker.id, rule.id) %>%
  summarise(num.pos = sum(polarity == "positive"),
            num.neg = sum(polarity == "negative"))

qplot(data = e.agg,
      facets = . ~ rule.id,
      x = num.pos,
      y = num.neg, alpha = I(0.6)) +
  theme_pub + 
  geom_abline() + 
  scale_x_continuous(name = '# positive examples', breaks = c(0, 5, 10), limits = c(0, 10)) +
  scale_y_continuous(name = '# negative examples', breaks = c(0, 5, 10), limits = c(0, 10))
```

## how related are the examples in edit distance?

```{r}
cluster.examples = function(strings, distance.threshold = 2) {
  distance.matrix = adist(strings)
  
  # for each string, figure out which other strings it's similar to
  # (i.e., has edit distance less than the threshold)
  similarities = apply(distance.matrix,
        1,# by row
        function(row) {
          which(row <= distance.threshold)
        })
  clusters = list()
  # print(similarities)
  
  # make the clusters
  Map(1:length(strings),
      f = function(i) {
        # j is the index of the previously created cluster that can contain this string
        j = Position(x = clusters,
                     f = function(cluster) { i %in% cluster })
        
        if (is.na(j)) {
          clusters[[length(clusters) + 1]] <<- similarities[[i]]
        } else {
          clusters[[j]] <<- union(clusters[[j]], similarities[[i]])
        }
      })
  
  Map(clusters,
      f = function(indices) { strings[indices] })
}

# # testing
# strings = c("01234", "012a4", "62804", "628041", "y6280", "0123", "280", "a280b")
# #strings = c('aaa','aa','aaab','baaaab','bbaaabb')
# ## strings = c("94301", "40510", "33333", "r2349", "asdfa", "3621", "834920")
# cluster.examples(strings)
```


```{r}
responses.clustered = responses %>%
  group_by(worker.id, rule.id) %>%
  summarise(num.examples = n(),
            num.clusters = length(cluster.examples(string)))

responses.clustered %>% group_by(worker.id) %>% summarise(mean_num.clusters = mean(num.clusters))

mean.num.clusters = mean(responses.clustered$num.clusters)
mean.num.clusters
```

TODO: do there tend to be more negative examples within a cluster? (i think people might come up with one example and then demonstrate various ways it can be perturbed to be a non-example)

comparison to permutation test: sample random participants by sampling from pool of all participants' responses (note that this is sampling *without* replacement, as people wouldn't give the same example twice)

```{r}
sample.bootstrap.subject = function(worker.id, rule.id) {
  # get examples given by all participants for this rule
  ## written in non-dplyr syntax because i think it might be faster?
  pool = responses[responses$rule.id == rule.id,]
  pool.pos = pool[pool$polarity == 'positive',]$string
  pool.neg = pool[pool$polarity == 'negative',]$string
  
  # get this worker's examples
  this = pool[pool$worker.id == worker.id,]
  
  num.pos = sum(this$polarity == 'positive')
  num.neg = sum(this$polarity == 'negative')
  
  syn.pos = sample(x = pool.pos, size = num.pos, replace = FALSE)
  syn.neg = sample(x = pool.neg, size = num.neg, replace = FALSE)
  
  c(num.clusters = length(cluster.examples(c(syn.pos, syn.neg))))
}

workers.and.rules = responses.clustered[,c('worker.id', 'rule.id')]

clusters.bootstrap = function() {
  num.clusters = apply(workers.and.rules,
                       1,
                       function(e) { 
                         sample.bootstrap.subject(e['worker.id'], e['rule.id'])
                       })
  
  mean(num.clusters)
}

# a little slow because my clustering function is not vectorized
system.time(bootstrap.samples <- replicate(1000, clusters.bootstrap()))['elapsed']

# one-tailed test: how many bootstrap samples have a mean
# number of clusters less than the observed sample?
sum(bootstrap.samples < mean.num.clusters) / length(bootstrap.samples)
```

## how many mistakes do people make? (e.g., positive examples that don't actually match or negative examples that do match)


by stimulus:

```{r}
responses %>%
  group_by(rule.id) %>%
  summarise(error.rate = sum(!correct) / n())
```

by person:
```{r}
responses %>%
  group_by(worker.id) %>%
  summarise(error.rate = sum(!correct) / n())
```


## how long are the examples that people give?

by stimulus:

```{r}
qplot(data = responses,
      x = rule.id,
      y = nchar(string),
      alpha = I(0.5))
```

by person:

```{r}
qplot(data = responses,
      x = worker.id,
      y = nchar(string), alpha = I(0.5))
```

## do people give examples in particular orders? e.g., shorter ones first or positive ones first?

length:
```{r}
e = responses %>%
  transform(len = nchar(string)) %>%
  group_by(example.num) %>%
  summarise(length.mean = mean(len),
            length.cl = generic.ci(len)['ci.l'],
            length.cu = generic.ci(len)['ci.u']) %>% ungroup

qplot(data = e,
      x = example.num,
      y = length.mean,
      ymin = length.cl,
      ymax = length.cu,
      geom = c('pointrange','line'))
```

polarity:

```{r}
e = responses %>%
        group_by(example.num) %>%
        summarise(frac.pos = sum(polarity == 'positive') / n(),
                  ci.l = generic.ci(polarity == 'positive')['ci.l'],
                  ci.u = generic.ci(polarity == 'positive')['ci.u'])

qplot(data = e,
      x = example.num,
      y = frac.pos,
      ymin = ci.l,
      ymax = ci.u,
      geom = c('pointrange','line'))
```

